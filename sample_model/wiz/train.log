Saved params to './sample_model/wiz/params.json'
Seq2Seq(
  (enc_embedder): Embedder(1564, 300, padding_idx=0)
  (encoder): RNNEncoder(
    (embedder): Embedder(1564, 300, padding_idx=0)
    (rnn): GRU(300, 128, batch_first=True, bidirectional=True)
  )
  (dec_embedder): Embedder(1564, 300, padding_idx=0)
  (kb_embedder): Embedder(1564, 300, padding_idx=0)
  (kbt_embedder): Embedder(1564, 300, padding_idx=0)
  (trans_layer): Linear(in_features=900, out_features=256, bias=True)
  (decoder): RNNDecoder(
    (embedder): Embedder(1564, 300, padding_idx=0)
    (dlg_attention): Attention(
      (rnn): GRU(512, 256, batch_first=True)
      (linear_query): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=256, bias=True)
      )
      (linear_memory): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=False)
        (1): Linear(in_features=256, out_features=256, bias=False)
        (2): Linear(in_features=256, out_features=256, bias=False)
      )
      (v): ModuleList(
        (0): Linear(in_features=256, out_features=1, bias=False)
        (1): Linear(in_features=256, out_features=1, bias=False)
        (2): Linear(in_features=256, out_features=1, bias=False)
      )
      (tanh): Tanh()
      (softmax): Softmax(dim=-1)
      (sigmoid): Sigmoid()
      (linear_forget): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=False)
        (1): Linear(in_features=256, out_features=256, bias=False)
        (2): Linear(in_features=256, out_features=256, bias=False)
      )
      (linear_add): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=False)
        (1): Linear(in_features=256, out_features=256, bias=False)
        (2): Linear(in_features=256, out_features=256, bias=False)
      )
    )
    (kb_attention): Attention(
      (rnn): GRU(512, 256, batch_first=True)
      (linear_query): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=256, bias=True)
      )
      (linear_memory): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=False)
        (1): Linear(in_features=256, out_features=256, bias=False)
        (2): Linear(in_features=256, out_features=256, bias=False)
      )
      (v): ModuleList(
        (0): Linear(in_features=256, out_features=1, bias=False)
        (1): Linear(in_features=256, out_features=1, bias=False)
        (2): Linear(in_features=256, out_features=1, bias=False)
      )
      (tanh): Tanh()
      (softmax): Softmax(dim=-1)
      (sigmoid): Sigmoid()
      (linear_forget): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=False)
        (1): Linear(in_features=256, out_features=256, bias=False)
        (2): Linear(in_features=256, out_features=256, bias=False)
      )
      (linear_add): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=False)
        (1): Linear(in_features=256, out_features=256, bias=False)
        (2): Linear(in_features=256, out_features=256, bias=False)
      )
    )
    (kb_memory_v3): KnowledgeMemoryv3(
      (dropout_layer): Dropout(p=0.0, inplace=False)
      (linear_query): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=True)
        (1): Linear(in_features=256, out_features=256, bias=True)
        (2): Linear(in_features=256, out_features=256, bias=True)
      )
      (linear_memory): ModuleList(
        (0): Linear(in_features=256, out_features=256, bias=False)
        (1): Linear(in_features=256, out_features=256, bias=False)
        (2): Linear(in_features=256, out_features=256, bias=False)
      )
      (v): ModuleList(
        (0): Linear(in_features=256, out_features=1, bias=False)
        (1): Linear(in_features=256, out_features=1, bias=False)
        (2): Linear(in_features=256, out_features=1, bias=False)
      )
      (tanh): Tanh()
      (C_0): Embedding(519, 256, padding_idx=0)
      (C_1): Embedding(519, 256, padding_idx=0)
      (C_2): Embedding(519, 256, padding_idx=0)
      (C_3): Embedding(519, 256, padding_idx=0)
      (softmax): Softmax(dim=-1)
      (sigmoid): Sigmoid()
    )
    (rnn): GRU(1068, 256, batch_first=True)
    (gate_layer): Sequential(
      (0): Linear(in_features=1324, out_features=1, bias=True)
      (1): Sigmoid()
    )
    (copy_gate_layer): Sequential(
      (0): Linear(in_features=512, out_features=1, bias=True)
      (1): Sigmoid()
    )
    (copy_gate_layer_kbt): Sequential(
      (0): Linear(in_features=256, out_features=1, bias=True)
      (1): Sigmoid()
    )
    (output_layer): Sequential(
      (0): Dropout(p=0.2, inplace=False)
      (1): Linear(in_features=1324, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=1564, bias=True)
      (3): Softmax(dim=-1)
    )
  )
  (sigmoid): Sigmoid()
  (src_attention): Attention(
    (rnn): GRU(512, 256, batch_first=True)
    (linear_query): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=256, bias=True)
      (2): Linear(in_features=256, out_features=256, bias=True)
    )
    (linear_memory): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=False)
      (1): Linear(in_features=256, out_features=256, bias=False)
      (2): Linear(in_features=256, out_features=256, bias=False)
    )
    (v): ModuleList(
      (0): Linear(in_features=256, out_features=1, bias=False)
      (1): Linear(in_features=256, out_features=1, bias=False)
      (2): Linear(in_features=256, out_features=1, bias=False)
    )
    (tanh): Tanh()
    (softmax): Softmax(dim=-1)
    (sigmoid): Sigmoid()
    (linear_forget): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=False)
      (1): Linear(in_features=256, out_features=256, bias=False)
      (2): Linear(in_features=256, out_features=256, bias=False)
    )
    (linear_add): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=False)
      (1): Linear(in_features=256, out_features=256, bias=False)
      (2): Linear(in_features=256, out_features=256, bias=False)
    )
  )
  (nll_loss): NLLLoss()
  (bce_loss): MaskBCELoss()
)
Number of parameters: 7855899

Training starts ...

=====================================================================================
================================== Model Training ===================================
=====================================================================================

[Train][ 1][1/1]   LOSS-TURN-1=104.090   ACC-TURN-1=0.025   LOSS-TURN-2=112.531   ACC-TURN-2=0.042   LOSS-TURN-3=149.922   ACC-TURN-3=0.024   LOSS-TURN-4=126.775   ACC-TURN-4=0.015   LOSS-TURN-5=107.204   ACC-TURN-5=0.011   TIME=4.41s
Saved model state to './sample_model/wiz/state_epoch_1.model'
Saved train state to './sample_model/wiz/state_epoch_1.train'


=====================================================================================
================================== Model Training ===================================
=====================================================================================

[Train][ 2][1/1]   LOSS-TURN-1=101.231   ACC-TURN-1=0.025   LOSS-TURN-2=109.925   ACC-TURN-2=0.042   LOSS-TURN-3=145.640   ACC-TURN-3=0.024   LOSS-TURN-4=123.042   ACC-TURN-4=0.015   LOSS-TURN-5=104.474   ACC-TURN-5=0.011   TIME=4.04s
Saved model state to './sample_model/wiz/state_epoch_2.model'
Saved train state to './sample_model/wiz/state_epoch_2.train'


=====================================================================================
================================== Model Training ===================================
=====================================================================================

